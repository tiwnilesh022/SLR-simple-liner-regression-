{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: \n",
    "Regression belongs to the class of Supervised Learning model where the datasets that are used for predictive/statistical modeling contain continuous labels. (A continuous variable is a variable that has an infinite number of possible values)\n",
    "\n",
    "### Linear Regression:\n",
    "Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range.\n",
    "- Simple Linear Regression\n",
    "- Multiple Linear Regression\n",
    "- Polynoial Linear Regression\n",
    "\n",
    "### Simple Linear Regression: \n",
    "These methods are used to predict the value of a target (dependent) variable from one or more features (independent) variables, where the variables are numeric.\"\n",
    "\n",
    "Simple Linear Regression is linear and continuous regression model.\n",
    "\n",
    "\n",
    "Here ultimate goal is, given a training set, to learn a function h:X→Y so that h(X) is a \"good\" predictor for the corresponding value of Y. The domain of values that both X and Y accept are set of all real numbers.\n",
    "<img src= './Image/5.1 Image a.png' width=200 height=200>\n",
    "Here, to predict accurately we have only m,b parameters to tune/adjust.\n",
    "\n",
    "- Our algorithm will try to learn the correct values for Weight and Bias. By the end of our training, our equation will approximate the line of best fit.\n",
    "- One prominent method seems to be to make y (actual value) close to mx(i)+b (predicted value), at least for the training examples you have.\n",
    "\n",
    "\n",
    "- For understanding let's try defining a function that determines, for each value of the m,b, how close the y's are to the corresponding (mx(i)+b)’s. The function should look like the following:\n",
    "<img src= './Image/5.1 Image b.jpg' width=300 height=300>\n",
    "\n",
    "    - Known as Cost function of simple linear regression\n",
    "    - y - actual value of an observation\n",
    "    - mx(i)+b - predicted value\n",
    "    - n is the total number of observations\n",
    "    - m - weight/coefficients weights is the coefficient for independent variable.\n",
    "    - b - bias/intercepts bias is the intercept where our line intercepts the y-axis. Bias offsets all predictions that we make.\n",
    "    \n",
    "    \n",
    "- MSE(Mean Sq. Error)(L2) measures the average squared difference between an observation’s actual and predicted values. \n",
    "  - The output is a single number representing the cost, or score, associated with our current set of weights.\n",
    "  - Our goal is to minimize MSE to improve the accuracy of our model.\n",
    "  - To minimize MSE we use Gradient Descent to calculate the gradient of our cost function.\n",
    "  -  There are two parameters (coefficients) in our cost function we can control: weight m and bias b.\n",
    "  - Since we need to consider the impact each one has on the final prediction, we use partial derivatives.\n",
    "\n",
    "### Summary:\n",
    "Simple Linear Regression (SLR) is a statistical method that examines the linear relationship between two continuous variables, X and Y.\n",
    "   - X is regarded as the independent variable while Y is regarded as the dependent variable. \n",
    "   - SLR discovers the best fitting line using Ordinary Least Squares (OLS) criterion.\n",
    "   - OLS criterion minimizes the sum of squared prediction error. \n",
    "   - Prediction error is defined as the difference between actual value y and predicted value mx(i)+b of dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient of determination:** Regression score function\n",
    "<img src='./Image/5.1 Image c.png' width= 300 height=100>\n",
    "i.e, ```R2 = 1-((SSE of line) / (SSE of y mean))```\n",
    "\n",
    "\n",
    "```SSE line = (y1-(mx1+b))+(y2-(mx2+b))+.....(yn-(mxn+b))```\n",
    "\n",
    "\n",
    "```SSE y mean = (y1-mean(y))+(y2-mean(y))+....(yn-mean(y))```\n",
    "\n",
    "- The R^2(or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination.\n",
    "- R2 score value range is between 0 to 1.\n",
    "- The higher the R-squared, the better the model fits your data.\n",
    "- If SSE line is small ===> R2 close to 1 i.e, line is a good fit.\n",
    "- If SSE line is large ===> R2 close to 0 i.e, line is not fitted well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
